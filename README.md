# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data from a banks telemarketing campaign. The objective of this project is to build a machine learning model which can identify whether a customer will subscribe to the marketing campaign.

The best performing classification model was found to be a voting ensemble which consisted of XGBoost, SGD and LightGBM models. This ensemble model had a classification accuracy of 91.8% which outperformed the logistic regression model which was optimised using Hyperdrive.

## Scikit-learn Pipeline
The model used was a logistic regression model which used hyperparameter tuning to tune the parameters for regularization strength and the maximum number of iterations.

The random sampling strategy has the advantage of being fast when compared to an exhaustive sampling method such as grid search. We can define N hyperparameter combinations to evaluate with.

The benefit of the bandit early stopping policy is that it prevents hyperdrive to continuously try and improve the hyperparameters when existing good values have been found. This leads to an increase in computational efficiency.

## AutoML
 The following configuration was used for training using AutoML:

 `automl_config = AutoMLConfig(experiment_timeout_minutes=30, compute_target=compute_target, task='classification', primary_metric='accuracy', training_data=dataset, label_column_name='y', n_cross_validations=3)`

Where:

experiment_timeout_minutes is the time you wish the experiment to run for.

compute_target is the AML compute target to run the Automated Machine Learning experiment.

task is whether this is a classification or regression task.

primary_metric is the metric used to track and optimise experiments.

training_dataset is the dataset that has been used to train models.

label_column_name is the prediction variable.

n_cross_validations is the number of cross validations used to ensure that the model generalises well.
 
 The model generated by AutoML was a voting ensemble which consisted of 10 models which included the following models: XGBoost, LightGBM, logistic regression, SGD.
 
  an XGBoost model and logistic regression model. Data was first scaled using the StandardScaling algorithm before being passed to the model.

The following hyperparameters have been used for the first model within the voting ensemble.


**XGBoost:**
| Hyper Parameter | Value |
|--------------|-----------|
|  colsample_bytree | 1
| eta | 0.05      |
| gamma      | 0  |
| max_depth | 6      |
| max_leaves      | 0  |
| n_estimators      | 200  |
| objective      | logistic regression  |
| reg_alpha | 0.625      |
| reg_lambda      |  0.8333333333333334  |
| subsample | 0.8      |
| tree_method      | auto  |

## Pipeline comparison
The Scikit-learn Pipeline is significantly more simple, consisting of just one logistic regression model which is not standardised. Whereas the AutoML voting ensemble uses an ensemble of 10 models, which have various preprocessing steps such as scaling strategies. The hyperdrive optimised pipeline had an accuracy of 91.0% whereas the AutoML pipeline produced a model with an accuracy of 91.8%.

## Future work
Future experiments could be improved by using bayesian hyperparameter optimization to efficiently find hyperparameters. Furthermore, varying the cross validation number of folds could be used to see if we can increase accuracy.
